# -*- coding: utf-8 -*-
"""tweet_sentiment (4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jrBfmeU00dnz0LOuk5byupn1Ji2Z5GJc
"""

# import libraries
import pandas as pd
import numpy as np

# libraries for data preprocessing
import nltk
# download modules available with NLTK
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

# libraries for data split and feature extraction
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# library for evaluation
from sklearn import metrics
from sklearn.metrics import ConfusionMatrixDisplay

# libraries for ML algorithms
from sklearn import svm
from sklearn import tree
from sklearn.naive_bayes import GaussianNB

# libraries for data plotting
import seaborn as sns
import matplotlib.pyplot as plt

RANDOM_SEED = 100

# Load our train and test csv
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

# Let's look at the first 5 rows of our training data
train.head()

train.info()

train.isna().sum()

train.duplicated().sum()

# Let's look at the first 5 rows of our test data
test.head()

test.isna().sum()

test.duplicated().sum()

# Summarise class details
sns.countplot(x=train['label'])

from nltk.tokenize import TweetTokenizer
from nltk.stem import WordNetLemmatizer
import re

tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)
lemmatizer = WordNetLemmatizer()

''' I created a function that helps in converting to lower case, remove special characters/numbers, tokenize our text
and lematize our text too '''
def clean_text(text):

    text = text.lower() # Convert to lowercase
    text = re.sub(r'\s+', ' ', text).strip() # Remove extra spaces
    text = re.sub(r'[^\w\s]', '', text) # removing puntuation

    tokens = tokenizer.tokenize(text)
    lemmatized_text = ' '.join([lemmatizer.lemmatize(token) for token in tokens])
    text = re.sub(r'[^a-zA-Z\s]', '', lemmatized_text) # Remove special characters and numbers

    stop_words = ['is', 'an', 'if', 'the', 'this', 'url', 'tcot', 'likes', 'we', 'a']
    words = [word for word in word_tokenize(text) if text not in stop_words]

    return text

# Let's apply the function on our train and test, remember anything you do on train you should do on test:
train['cleaned_tweet'] = train['tweet'].apply(clean_text)
test['cleaned_tweet'] = test['tweet'].apply(clean_text)

# Let's see our first 5 rows
train.head()

X = train['cleaned_tweet']
y = train['label']

test.head()

# Let's split our data
# use lemmatised text
X_train_lemmatised_text, X_val_lemmatised_text, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=train['label'])

def evaluate(actuals, predictions, class_names):
  '''
  actuals: list of actual labels
  predictions: list of predicted labels
  class_names: list of classes used for the model/classification task
  '''
  # calculate accuracy
  accuracy = metrics.accuracy_score(actuals, predictions)
  print(f'Accuracy: {accuracy}')

  # plot confusion matrix
  confusion_matrix = metrics.confusion_matrix(actuals, predictions, labels=class_names)
  disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=class_names)
  disp.plot()
  plt.show()

# from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# method to train and predict using SVM classifier
def get_svm_predictions(X_train, X_val, y_train, y_val):
  # build model with Support Vector Machine classifier

    from sklearn.utils.class_weight import compute_class_weight

    # adding the class weight of the classes in other to balance the classes
    class_labels = np.unique(y_train)
    class_weights = compute_class_weight('balanced', classes=class_labels, y=y_train)

    # Convert class weights to dictionary format
    class_weight = {class_labels[i]: class_weights[i] for i in range(len(class_labels))}


    clf = SVC(kernel='rbf', C=1000, gamma=0.3, probability=True, class_weight=class_weight, random_state=42)
    clf.fit(X_train, y_train)

    # Make predictions on test data
    y_pred = clf.predict(X_val)

    # evalution
    evaluate(y_val, y_pred, clf.classes_)

    return clf

vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True) # converting word to number using the tfidf algorithm  # default: lowercase=True, ngram_range=(1,1)
vectorizer.fit(X_train_lemmatised_text)

# convert train and test text data to numeric vectors
X_train = vectorizer.transform(X_train_lemmatised_text)
X_val = vectorizer.transform(X_val_lemmatised_text)

m1 = get_svm_predictions(X_train, X_val, y_train, y_val)

# Test set
# Convert the test set to numeric vectors
test_vector = vectorizer.transform(test['cleaned_tweet'])

#Predict on the numeric vectors for the test
test_prediction = m1.predict(test_vector)

test['prediction'] = test_prediction

test[['id', 'prediction']].to_json('submission1.json', orient='records', lines=True)

test